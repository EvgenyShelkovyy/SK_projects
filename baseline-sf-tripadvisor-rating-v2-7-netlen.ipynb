{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n","metadata":{}},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nimport re\nimport missingno as msno\nsns.set()\n\nfrom collections import Counter\nfrom datetime import datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option(\"display.max_rows\", 250)\npd.set_option(\"display.max_columns\", 250)\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-10-24T11:24:11.978874Z","iopub.execute_input":"2021-10-24T11:24:11.979491Z","iopub.status.idle":"2021-10-24T11:24:11.997062Z","shell.execute_reply.started":"2021-10-24T11:24:11.979139Z","shell.execute_reply":"2021-10-24T11:24:11.996383Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:11.998375Z","iopub.execute_input":"2021-10-24T11:24:11.998724Z","iopub.status.idle":"2021-10-24T11:24:12.009970Z","shell.execute_reply.started":"2021-10-24T11:24:11.998688Z","shell.execute_reply":"2021-10-24T11:24:12.009045Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:12.011429Z","iopub.execute_input":"2021-10-24T11:24:12.011812Z","iopub.status.idle":"2021-10-24T11:24:14.320228Z","shell.execute_reply.started":"2021-10-24T11:24:12.011766Z","shell.execute_reply":"2021-10-24T11:24:14.318542Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTIONS","metadata":{}},{"cell_type":"code","source":"# Create a list with restaurants which might be in chain\ndef show_chained_rest(data):\n    chained_rest_list = list(data['Restaurant_id'].value_counts()[data['Restaurant_id'].value_counts() > 1].index)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.322745Z","iopub.execute_input":"2021-10-24T11:24:14.323237Z","iopub.status.idle":"2021-10-24T11:24:14.329447Z","shell.execute_reply.started":"2021-10-24T11:24:14.323016Z","shell.execute_reply":"2021-10-24T11:24:14.327817Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"#  Для определения границ выбросов\ndef func_IQR(data, column):\n    perc_25 = data[column].quantile(0.25, interpolation=\"midpoint\")\n    perc_75 = data[column].quantile(0.75, interpolation=\"midpoint\")\n    IQR = perc_75 - perc_25\n    print('Q1: {}'.format(perc_25), 'Q3: {}'.format(perc_75), 'IQR: {}'.format(IQR),\n          'Emission limit: [{a},{b}]'.format(a=perc_25 - 1.5*IQR, b=perc_75 + 1.5*IQR), sep='\\n')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.332134Z","iopub.execute_input":"2021-10-24T11:24:14.332516Z","iopub.status.idle":"2021-10-24T11:24:14.343973Z","shell.execute_reply.started":"2021-10-24T11:24:14.332478Z","shell.execute_reply":"2021-10-24T11:24:14.342835Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Processing the Cuisine Style column\n# We transform the string with kitchens into a list of kitchens, for this we create a function\ndef str_to_list(row_list):\n    if pd.isna(row_list): return ['unknown']\n    row_list = row_list.replace(\"\\'\",'').strip('[]').split(', ')\n    return row_list\n\n# Add a column with the number of kitchens represented\ndef len_row(row): # counting the cuisines presented by the restaurant\n    x = len(row)\n    return x\n\n    data['Cuisines'] = data['Cuisine Style'].apply(len_row)\n\n# Adding a column for the availability of vegetarian food\ndef vegan(date):\n    word = 'Veg'\n    i=0\n    for item in date:\n        if word in item:\n            i+=1\n    return i\n\n    data['Vegenarian_options'] = data['Cuisine Style'].apply(vegan)\n\n# Add a column with the presence of Wine Bar or Pab\ndef wine(date):\n    word = ['Wine Bar', 'Pub', 'Bar']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Wine_pub'] = data['Cuisine Style'].apply(wine)\n\n# Add a column with Delicatessen\ndef delicatessen(date):\n    word = 'Delicatessen'\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['delicatessen_pub'] = data['Cuisine Style'].apply(delicatessen)\n\n# Add a column with the presence of Fast Food and Street Food\ndef fastfood(date):\n    word = ['Fast Food', 'Street Food']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Fastfood'] = data['Cuisine Style'].apply(fastfood)\n\n# Add a column with Sushi, Japanese\ndef sushi(date):\n    word = ['Japanese', 'Sushi']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Sushi'] = data['Cuisine Style'].apply(sushi)\n\n# Add a column with Grill, Barbecue\ndef grill(date):\n    word = ['Grill', 'Barbecue']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Barbecue'] = data['Cuisine Style'].apply(grill)\n\n# Add a column with the presence of Italian food\ndef italian(date):\n    word = ['Pizza', 'Italian']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Italian_cuisine'] = data['Cuisine Style'].apply(italian)\n\n# Add a column with the presence of European cuisine\ndef europe(date):\n    word = 'Europe'\n    i=0\n    for item in date:\n        if word in item:\n            i+=1\n    return i\n\n    data['Europe_cuisine'] = data['Cuisine Style'].apply(europe)\n\n# Add a column with the presence of south american cuisines\ndef south_american(date):\n    word = ['Mexican', 'Venezuelan', 'South American', 'Brasilian']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['South_american_cuisine'] = data['Cuisine Style'].apply(south_american)\n\n# Adding a column for Asian food\ndef asian(date):\n    word = ['Asian', 'Thai', 'Vietnamese']\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Asian_cuisine'] = data['Cuisine Style'].apply(asian)\n    \n# Add a healthy kitchen column\ndef healthy(date):\n    word = 'Healthy'\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Healthy_cuisine'] = data['Cuisine Style'].apply(healthy)\n\n# Adding a column with the presence of halal food\ndef halal(date):\n    word = 'Halal'\n    i=0\n    for item in date:\n        if item in word:\n            i+=1\n    return i\n\n    data['Halal_cuisine'] = data['Cuisine Style'].apply(halal)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.347129Z","iopub.execute_input":"2021-10-24T11:24:14.347702Z","iopub.status.idle":"2021-10-24T11:24:14.369692Z","shell.execute_reply.started":"2021-10-24T11:24:14.347654Z","shell.execute_reply":"2021-10-24T11:24:14.368781Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Process the Reviews column\n# Add a column with a positive feedback tone\n# To determine the conditionally positive tone of the review\ndef str_to_list_rev(row_list):\n    if pd.isna(row_list): return ['unknown']\n    row_list = row_list.replace(\"\\'\",'').strip('[]').split(', ')\n    return row_list\n\ndef good_rev(data):\n    i = 0\n    for item in data:\n        checklist = {'good', 'nice', 'excellent', 'best', 'wonderful', 'unique', 'delicious','heavenly', 'amazing', 'brilliant', 'great place', 'better'}\n        common_words = set(item.lower().split()) & checklist\n        i += len(common_words)\n    return i\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.371515Z","iopub.execute_input":"2021-10-24T11:24:14.372056Z","iopub.status.idle":"2021-10-24T11:24:14.388518Z","shell.execute_reply.started":"2021-10-24T11:24:14.371950Z","shell.execute_reply":"2021-10-24T11:24:14.387233Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Replace the values of the categorical attribute Price Range with categories 1-3\ndef price_range(data):\n    range_map = {'$':1, '$$ - $$$':2, '$$$$':3}\n    data['Price Range'] = data['Price Range'].map(range_map)\n    x_price = data['Price Range'].median()\n    data['Price Range'].fillna(x_price, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.390360Z","iopub.execute_input":"2021-10-24T11:24:14.391041Z","iopub.status.idle":"2021-10-24T11:24:14.403800Z","shell.execute_reply.started":"2021-10-24T11:24:14.390738Z","shell.execute_reply":"2021-10-24T11:24:14.402785Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Add a sign of the difference in days between the submitted reviews\ndef days_bet_rev(data):\n    pattern = re.compile('\\d+\\/\\d+\\/\\d+')\n    data['reviews_date'] = data['Reviews'].apply(lambda x: pattern.findall(x))\n    \n    def last_reviews (row): # Last_reviews column fill function\n        if len(row) != 0:\n            return row[0]\n        else:\n            return 0\n\n    def first_reviews (row): # Function to fill the column first_reviews\n        if len(row) != 0:\n            return row[-1]\n        else:\n            return 0\n# Add a sign of the difference in seconds between reviews\n    data['first_reviews'] = data['reviews_date'].apply(first_reviews)\n    data['last_reviews'] = data['reviews_date'].apply(last_reviews)\n    data['last_reviews'] = pd.to_datetime(data['last_reviews']).dt.date\n    data['first_reviews'] = pd.to_datetime(data['first_reviews']).dt.date\n    data['days_bet_rev'] = data['last_reviews'] - data['first_reviews']\n    data['days_bet_rev'] = np.abs(data['days_bet_rev'].dt.total_seconds()/86400)\n# Delete temporary columns \n    data.drop(['last_reviews', 'first_reviews', 'reviews_date'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.407387Z","iopub.execute_input":"2021-10-24T11:24:14.408596Z","iopub.status.idle":"2021-10-24T11:24:14.420231Z","shell.execute_reply.started":"2021-10-24T11:24:14.408530Z","shell.execute_reply":"2021-10-24T11:24:14.419030Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Additional signs\n# Index of purchasing power according to Numbeo rating (2018)\ndef signs_for_cities(data):\n    purchasing_power_index = {\n        'Amsterdam' : 19,\n        'Athens' : 53,\n        'Barcelona' : 30,\n        'Berlin' : 8,\n        'Bratislava' : 39,\n        'Brussels' : 15,\n        'Budapest' : 51,\n        'Copenhagen' : 10,\n        'Dublin' : 27,\n        'Edinburgh' : 16,\n        'Geneva' : 3,\n        'Hamburg' : 7,\n        'Helsinki' : 9,\n        'Krakow' : 36,\n        'Lisbon' : 48,\n        'Ljubljana' : 32,\n        'London' : 24,\n        'Luxembourg' : 4,\n        'Lyon' : 70,\n        'Madrid' : 23,\n        'Milan' : 40,\n        'Munich' : 6,\n        'Oporto' : 42,\n        'Oslo' : 17,\n        'Paris' : 22,\n        'Prague' : 31,\n        'Rome' : 37,\n        'Stockholm' : 13,\n        'Vienna' : 21,\n        'Warsaw' : 29,\n        'Zurich' : 2\n    }\n\n    data['purchasing_power_index'] = data['City'].map(purchasing_power_index)\n    \n# New trait - The number of inhabitants in the city, according to the Wiki\n    population_dict = {'Paris': 2240621, 'Stockholm': 1981263, 'London': 8173900,\\\n                       'Berlin': 3326002, 'Munich': 1561094, 'Oporto': 258975,\\\n                       'Milan': 1331586, 'Bratislava': 413192, 'Vienna': 1765649,\\\n                       'Rome': 2870493, 'Barcelona': 1593075, 'Madrid': 3155360,\\\n                       'Dublin': 506211, 'Brussels': 144784, 'Zurich': 402275,\\\n                       'Warsaw': 1720398, 'Budapest': 1744665, 'Copenhagen': 1246611,\\\n                       'Amsterdam': 825080, 'Lyon': 496343, 'Hamburg':  1718187,\\\n                       'Lisbon': 547733, 'Prague': 1335084, 'Oslo': 673469,\\\n                       'Helsinki': 574579, 'Edinburgh': 468070,\\\n                       'Geneva': 196150, 'Ljubljana': 277554, 'Athens': 3168846,\\\n                       'Luxembourg': 648538, 'Krakow': 756183}\n    data['population'] = data['City'].map(population_dict)\n    \n\n# Total restaurants in the city\n    count_rest_dict = {'Paris': 15633, 'Stockholm': 2783, 'London': 17239,\\\n                       'Berlin': 5635, 'Munich': 2882, 'Oporto': 1861,\\\n                       'Milan': 6750, 'Bratislava': 1045, 'Vienna': 3803,\\\n                       'Rome': 10360, 'Barcelona': 8726, 'Madrid': 10348,\\\n                       'Dublin': 2107, 'Brussels': 2365, 'Zurich': 1676,\\\n                       'Warsaw': 2788, 'Budapest': 2786, 'Copenhagen': 1991,\\\n                       'Amsterdam': 3671, 'Lyon': 2574, 'Hamburg':  20457,\\\n                       'Lisbon': 4576, 'Prague': 4653, 'Oslo': 1190,\\\n                       'Helsinki': 1388, 'Edinburgh': 1773, 'Geneva': 1346,\\\n                       'Ljubljana': 542, 'Athens': 2362,\\\n                       'Luxembourg': 659, 'Krakow': 1535}\n    data['all_rest'] = data['City'].map(count_rest_dict)\n\n# Average salary in the country\n    count_salary_dict = {'Paris': 3332, 'Stockholm': 2893, 'London': 2703,\\\n                         'Berlin': 4392, 'Munich': 4392, 'Oporto': 1288,\\\n                         'Milan': 2726, 'Bratislava': 1283, 'Vienna': 2940,\n                         'Rome': 2726, 'Barcelona': 2133, 'Madrid': 2133,\\\n                         'Dublin': 2107, 'Brussels': 3930, 'Zurich': 6244,\\\n                         'Warsaw': 2788, 'Budapest': 2786, 'Copenhagen': 6192,\\\n                         'Amsterdam': 3238, 'Lyon': 3332, 'Hamburg':  4392,\\\n                         'Lisbon': 1288, 'Prague': 1354, 'Oslo': 5450,\\\n                         'Helsinki': 3908, 'Edinburgh': 2703, 'Geneva': 6244,\\\n                         'Ljubljana': 1914, 'Athens': 1203,'Luxembourg': 5854,\\\n                         'Krakow': 1253}\n    data['average_salary'] = data['City'].map(count_salary_dict)\n    \n# Tourist attraction.\n    # Place of the city in the ranking of the top 100 cities visited in 2018\n    # Source - Euromonitor international\n    # Since not all cities are in the top 100, we divide cities into groups:\n    # 1 - place 1-25, 2 - place 26-50, 3 - place 51-75\n    # 4 - place 76-100, 5 - place above 100\n\n    tourist_attract = {'London' : 1, 'Paris' : 1, 'Madrid' : 2, 'Barcelona' : 2,\\\n        'Berlin' : 2, 'Milan' : 5, 'Rome' : 1, 'Prague' : 1, 'Lisbon' : 3, 'Vienna' : 2,\\\n        'Amsterdam' : 1, 'Brussels' : 3, 'Hamburg' : 5, 'Munich' : 3, 'Lyon' : 5,\\\n        'Stockholm' : 4, 'Budapest' : 3, 'Warsaw' : 3, 'Dublin' : 5, 'Copenhagen' : 3,\\\n        'Athens' : 2, 'Edinburgh' : 5, 'Zurich' : 5, 'Oporto' : 4, 'Geneva' : 5, \\\n        'Krakow' : 3, 'Oslo' : 5, 'Helsinki' : 5, 'Bratislava' : 5, 'Luxembourg' : 5,\\\n        'Ljubljana' : 5\n    }\n\n    data['tourist_attract'] = data['City'].map(tourist_attract)  ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.423244Z","iopub.execute_input":"2021-10-24T11:24:14.423894Z","iopub.status.idle":"2021-10-24T11:24:14.446458Z","shell.execute_reply.started":"2021-10-24T11:24:14.423832Z","shell.execute_reply":"2021-10-24T11:24:14.445407Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# function for standardization\ndef StandardScaler_column(data, d_col):\n    scaler = StandardScaler()\n    scaler.fit(data[[d_col]])\n    return scaler.transform(data[[d_col]])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.449372Z","iopub.execute_input":"2021-10-24T11:24:14.449915Z","iopub.status.idle":"2021-10-24T11:24:14.459314Z","shell.execute_reply.started":"2021-10-24T11:24:14.449854Z","shell.execute_reply":"2021-10-24T11:24:14.458608Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Distribution of values ​​between Ranking and Raiting are similar.\n# When restaurants with the highest Ranking get 4-4.5\n# Create a feature that calculates the Rating value based on Ranking\n# Compare the rank to the number of restaurants in the city.\n# Normalize the data so that the rank does not depend on the number of restaurants in the city.\ndef ranking_normalize(data):\n    mean_per_city = data.groupby('City')['Ranking'].mean()\n    data['mean_ranking_per_city'] = data['City'].apply(lambda x: mean_per_city[x])\n    max_per_city = data.groupby('City')['Ranking'].max()\n    data['max_ranking_per_city'] = data['City'].apply(lambda x: max_per_city[x])\n    data['standard_ranking'] = (data['Ranking'] - data['mean_ranking_per_city']) / data['max_ranking_per_city']\n    data.drop(['mean_ranking_per_city', 'max_ranking_per_city'], axis = 1, inplace=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.460483Z","iopub.execute_input":"2021-10-24T11:24:14.460877Z","iopub.status.idle":"2021-10-24T11:24:14.472707Z","shell.execute_reply.started":"2021-10-24T11:24:14.460820Z","shell.execute_reply":"2021-10-24T11:24:14.471782Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-10-24T11:24:14.474595Z","iopub.execute_input":"2021-10-24T11:24:14.475036Z","iopub.status.idle":"2021-10-24T11:24:14.765346Z","shell.execute_reply.started":"2021-10-24T11:24:14.474836Z","shell.execute_reply":"2021-10-24T11:24:14.764012Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.767834Z","iopub.execute_input":"2021-10-24T11:24:14.768194Z","iopub.status.idle":"2021-10-24T11:24:14.793756Z","shell.execute_reply.started":"2021-10-24T11:24:14.768139Z","shell.execute_reply":"2021-10-24T11:24:14.792529Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.795625Z","iopub.execute_input":"2021-10-24T11:24:14.796026Z","iopub.status.idle":"2021-10-24T11:24:14.814750Z","shell.execute_reply.started":"2021-10-24T11:24:14.795953Z","shell.execute_reply":"2021-10-24T11:24:14.813670Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.816446Z","iopub.execute_input":"2021-10-24T11:24:14.816822Z","iopub.status.idle":"2021-10-24T11:24:14.833314Z","shell.execute_reply.started":"2021-10-24T11:24:14.816753Z","shell.execute_reply":"2021-10-24T11:24:14.832502Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"df_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.834316Z","iopub.execute_input":"2021-10-24T11:24:14.834690Z","iopub.status.idle":"2021-10-24T11:24:14.851557Z","shell.execute_reply.started":"2021-10-24T11:24:14.834651Z","shell.execute_reply":"2021-10-24T11:24:14.850760Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.852613Z","iopub.execute_input":"2021-10-24T11:24:14.853067Z","iopub.status.idle":"2021-10-24T11:24:14.862642Z","shell.execute_reply.started":"2021-10-24T11:24:14.853020Z","shell.execute_reply":"2021-10-24T11:24:14.861939Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"sample_submission.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.863958Z","iopub.execute_input":"2021-10-24T11:24:14.864291Z","iopub.status.idle":"2021-10-24T11:24:14.879739Z","shell.execute_reply.started":"2021-10-24T11:24:14.864238Z","shell.execute_reply":"2021-10-24T11:24:14.879040Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# IMPORTANT! For correct processing of features, we combine the train and test into one dataset\ndf_train['sample'] = 1 # mark where we have train\ndf_test['sample'] = 0 # mark where we have the test\ndf_test['Rating'] = 0 # in the test we do not have a Rating value, we have to predict it, so for now just fill in with zeros\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # combine","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.881023Z","iopub.execute_input":"2021-10-24T11:24:14.881418Z","iopub.status.idle":"2021-10-24T11:24:14.928044Z","shell.execute_reply.started":"2021-10-24T11:24:14.881377Z","shell.execute_reply":"2021-10-24T11:24:14.927282Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"df = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.929050Z","iopub.execute_input":"2021-10-24T11:24:14.929395Z","iopub.status.idle":"2021-10-24T11:24:14.954697Z","shell.execute_reply.started":"2021-10-24T11:24:14.929358Z","shell.execute_reply":"2021-10-24T11:24:14.954004Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана","metadata":{}},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.955732Z","iopub.execute_input":"2021-10-24T11:24:14.956018Z","iopub.status.idle":"2021-10-24T11:24:14.977014Z","shell.execute_reply.started":"2021-10-24T11:24:14.955967Z","shell.execute_reply":"2021-10-24T11:24:14.975934Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"df.Reviews[1]","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.978371Z","iopub.execute_input":"2021-10-24T11:24:14.978656Z","iopub.status.idle":"2021-10-24T11:24:14.986280Z","shell.execute_reply.started":"2021-10-24T11:24:14.978608Z","shell.execute_reply":"2021-10-24T11:24:14.985197Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"As you can see, most of our signs require cleaning and preliminary processing.","metadata":{}},{"cell_type":"markdown","source":"# Cleaning and Prepping Data\nTypically, the data contains a bunch of garbage that needs to be cleaned up in order to bring it into an acceptable format. Data cleansing is a necessary step in solving almost any real-world problem.  \n![](https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png)","metadata":{}},{"cell_type":"code","source":"# Show number of missing percentages\ndisplay(df.isnull().sum()/(((df.index[-1])+1)/100))\n\n# more clearly it is displayed on the graph\nmsno.bar(data, figsize=(18, 6),  fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:14.987814Z","iopub.execute_input":"2021-10-24T11:24:14.988127Z","iopub.status.idle":"2021-10-24T11:24:15.655607Z","shell.execute_reply.started":"2021-10-24T11:24:14.988076Z","shell.execute_reply":"2021-10-24T11:24:15.654329Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"data.Restaurant_id.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.657184Z","iopub.execute_input":"2021-10-24T11:24:15.657446Z","iopub.status.idle":"2021-10-24T11:24:15.679733Z","shell.execute_reply.started":"2021-10-24T11:24:15.657398Z","shell.execute_reply":"2021-10-24T11:24:15.679124Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"We see a lot of duplicate ids, probably chain restaurants.","metadata":{}},{"cell_type":"markdown","source":"## 1. NAN processing\nThere can be various reasons for the presence of gaps, but gaps must be either filled in or excluded from the set entirely. But you need to be careful with gaps, ** even the lack of information can be an important sign! **\nTherefore, before processing NAN, it is better to take out information about the presence of a pass as a separate sign.","metadata":{}},{"cell_type":"code","source":"# I'll take the Number of Reviews, Cuisine Style, Price Range column as an example\ndf['number_of_reviews_isNAN'] = pd.isna(df['Number of Reviews']).astype('uint8')\ndf['cuisine_style_isNAN'] = pd.isna(df['Cuisine Style']).astype('uint8')\ndf['price_range_isNAN'] = pd.isna(df['Price Range']).astype('uint8')\ndf['reviews_isNAN'] = pd.isna(df['Reviews']).astype('uint8')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.681036Z","iopub.execute_input":"2021-10-24T11:24:15.681299Z","iopub.status.idle":"2021-10-24T11:24:15.704054Z","shell.execute_reply.started":"2021-10-24T11:24:15.681253Z","shell.execute_reply":"2021-10-24T11:24:15.702262Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"df['number_of_reviews_isNAN']","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.705584Z","iopub.execute_input":"2021-10-24T11:24:15.706034Z","iopub.status.idle":"2021-10-24T11:24:15.713262Z","shell.execute_reply.started":"2021-10-24T11:24:15.705978Z","shell.execute_reply":"2021-10-24T11:24:15.712164Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Next, fill in the gaps 0, you can try filling with the average or average for the city, etc.\ndf['Number of Reviews'].fillna((df['Number of Reviews'].mean()), inplace=True)# Show number of missing percentages\n#df['Reviews'].fillna(('[], []]'), inplace=True)# Show number of missing percentages\ndf.isnull().sum()/(((df.index[-1])+1)/100)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.714325Z","iopub.execute_input":"2021-10-24T11:24:15.714896Z","iopub.status.idle":"2021-10-24T11:24:15.745958Z","shell.execute_reply.started":"2021-10-24T11:24:15.714791Z","shell.execute_reply":"2021-10-24T11:24:15.745122Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"### 2. Processing characteristics\nFirst, let's see what signs we can have categorical.","metadata":{}},{"cell_type":"code","source":"df.nunique(dropna=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.747274Z","iopub.execute_input":"2021-10-24T11:24:15.747551Z","iopub.status.idle":"2021-10-24T11:24:15.851366Z","shell.execute_reply.started":"2021-10-24T11:24:15.747501Z","shell.execute_reply":"2021-10-24T11:24:15.850595Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"What signs can be considered categorical?\nCity, Price Range","metadata":{}},{"cell_type":"markdown","source":"There are many approaches to coding categorical features:\n* Label Encoding\n* One-Hot Encoding\n* Target Encoding\n* Hashing\n\nThe choice of coding depends on the characteristic and the chosen model.\nLet's not dive deeply into this topic now, let's see a better example with One-Hot Encoding:\n![](https://i.imgur.com/mtimFxh.png)","metadata":{}},{"cell_type":"code","source":"# For One-Hot Encoding, pandas has a ready-made function - get_dummies. The parameter dummy_na is especially pleasing.\ndf = pd.get_dummies(df, columns=[ 'City',], dummy_na=True)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.852768Z","iopub.execute_input":"2021-10-24T11:24:15.853069Z","iopub.status.idle":"2021-10-24T11:24:15.917520Z","shell.execute_reply.started":"2021-10-24T11:24:15.853019Z","shell.execute_reply":"2021-10-24T11:24:15.916369Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.921660Z","iopub.execute_input":"2021-10-24T11:24:15.921953Z","iopub.status.idle":"2021-10-24T11:24:15.953970Z","shell.execute_reply.started":"2021-10-24T11:24:15.921902Z","shell.execute_reply":"2021-10-24T11:24:15.953142Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.955344Z","iopub.execute_input":"2021-10-24T11:24:15.955624Z","iopub.status.idle":"2021-10-24T11:24:15.994982Z","shell.execute_reply.started":"2021-10-24T11:24:15.955573Z","shell.execute_reply":"2021-10-24T11:24:15.994245Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"#### Cuisine Style\nLet's convert the values ​​of the Cuisine Style column to strings for further data processing.","metadata":{}},{"cell_type":"code","source":"df['Cuisine Style'] = df['Cuisine Style'].apply(str_to_list_rev)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:15.996288Z","iopub.execute_input":"2021-10-24T11:24:15.996741Z","iopub.status.idle":"2021-10-24T11:24:16.228868Z","shell.execute_reply.started":"2021-10-24T11:24:15.996699Z","shell.execute_reply":"2021-10-24T11:24:16.228029Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"#### Let's take the next attribute \"Price Range\".","metadata":{}},{"cell_type":"code","source":"df['Price Range'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:16.230019Z","iopub.execute_input":"2021-10-24T11:24:16.230396Z","iopub.status.idle":"2021-10-24T11:24:16.244605Z","shell.execute_reply.started":"2021-10-24T11:24:16.230357Z","shell.execute_reply":"2021-10-24T11:24:16.243006Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"### Processing the rest of the signs","metadata":{}},{"cell_type":"code","source":"# Create a dummy attribute from the \"Price Range\" categorical column\n#df = pd.get_dummies(data=df, columns=['Price Range'], dummy_na=True)\n# Create a 'price range' dictionary and replace the values in the current column\nrange_map = {'$':1, '$$ - $$$':2, '$$$$':3}\ndf['Price Range'] = df['Price Range'].map(range_map)\n\n# fill in the gaps with median values\ndf['Price Range'].fillna((df['Price Range'].median()), inplace=True)\n\ndf.head()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:16.246642Z","iopub.execute_input":"2021-10-24T11:24:16.247055Z","iopub.status.idle":"2021-10-24T11:24:16.298107Z","shell.execute_reply.started":"2021-10-24T11:24:16.246981Z","shell.execute_reply":"2021-10-24T11:24:16.295999Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:16.299905Z","iopub.execute_input":"2021-10-24T11:24:16.300515Z","iopub.status.idle":"2021-10-24T11:24:16.333307Z","shell.execute_reply.started":"2021-10-24T11:24:16.300451Z","shell.execute_reply":"2021-10-24T11:24:16.331980Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# CONVERT REVIEWS\n# Make a pattern to extract dates from the Reviews string and create new features last_reviews and first_reviews\n# We make a pattern to extract dates from the Reviews string and create new features\ndf['Reviews'].fillna('[Unknown]', inplace=True)\npattern = re.compile('\\d+\\/\\d+\\/\\d+')\ndf['reviews_date'] = df['Reviews'].apply(lambda x: pattern.findall(x))\n\n\ndef last_reviews (row): # Function of the last_reviews\n    if len(row) != 0:\n        return row[0]\n    else:\n        return 0\n\n    \ndef first_reviews (row): # Function of the first_reviews\n    if len(row) != 0:\n        return row[-1]\n    else:\n        return 0\n    \n    \ndf['first_reviews'] = df['reviews_date'].apply(first_reviews)\ndf['last_reviews'] = df['reviews_date'].apply(last_reviews)\n\n# New feature - positive sentiment of reviews\ndf['reviews_temp'] = df['Reviews'].apply(str_to_list)\n\ndef good(date):\n    i = 0\n    for item in date:\n        checklist = {'good', 'nice', 'excellent', 'best', 'wonderful', 'unique', 'delicious','heavenly', 'amazing', 'brilliant', 'great place', 'better'}\n        common_words = set(item.lower().split()) & checklist\n        i += len(common_words)\n    return i\n\ndf['good_reviews'] = df['reviews_temp'].apply(good)\n\n# New feature - with the difference in days between reviews\ndf['last_reviews'] = pd.to_datetime(df['last_reviews']).dt.date\ndf['first_reviews'] = pd.to_datetime(df['first_reviews']).dt.date\ndf['date_def'] = df['last_reviews'] - df['first_reviews']\ndf['date_def'] = df['date_def'].dt.total_seconds()/86400","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:16.334662Z","iopub.execute_input":"2021-10-24T11:24:16.334935Z","iopub.status.idle":"2021-10-24T11:24:17.265682Z","shell.execute_reply.started":"2021-10-24T11:24:16.334884Z","shell.execute_reply":"2021-10-24T11:24:17.264575Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:17.267213Z","iopub.execute_input":"2021-10-24T11:24:17.267479Z","iopub.status.idle":"2021-10-24T11:24:17.332737Z","shell.execute_reply.started":"2021-10-24T11:24:17.267432Z","shell.execute_reply":"2021-10-24T11:24:17.331479Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"![](https://cs10.pikabu.ru/post_img/2018/09/06/11/1536261023140110012.jpg)","metadata":{}},{"cell_type":"markdown","source":"# EDA \n[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Data analysis\nAt this stage, we build graphs, look for patterns, anomalies, outliers or connections between signs.\nIn general, the purpose of this stage is to understand what this data can give us and how the signs can be interrelated.\nUnderstanding the original features will allow us to generate new, stronger ones and, thereby, make our model better.\n![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png)","metadata":{}},{"cell_type":"markdown","source":"### Let's see the distribution of the characteristic","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:17.334354Z","iopub.execute_input":"2021-10-24T11:24:17.334661Z","iopub.status.idle":"2021-10-24T11:24:17.812081Z","shell.execute_reply.started":"2021-10-24T11:24:17.334596Z","shell.execute_reply":"2021-10-24T11:24:17.810448Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"We have a lot of restaurants that do not even hold up to 2500 seats in their city, but what about the cities?","metadata":{}},{"cell_type":"code","source":"df_train['City'].value_counts(ascending=True).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:17.813458Z","iopub.execute_input":"2021-10-24T11:24:17.813712Z","iopub.status.idle":"2021-10-24T11:24:18.251172Z","shell.execute_reply.started":"2021-10-24T11:24:17.813667Z","shell.execute_reply":"2021-10-24T11:24:18.250107Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"And someone said that the French love to eat =) Let's see how the distribution in a big city will change:","metadata":{}},{"cell_type":"code","source":"df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:18.252629Z","iopub.execute_input":"2021-10-24T11:24:18.252906Z","iopub.status.idle":"2021-10-24T11:24:18.740806Z","shell.execute_reply.started":"2021-10-24T11:24:18.252859Z","shell.execute_reply":"2021-10-24T11:24:18.739647Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:18.743023Z","iopub.execute_input":"2021-10-24T11:24:18.743429Z","iopub.status.idle":"2021-10-24T11:24:21.276007Z","shell.execute_reply.started":"2021-10-24T11:24:18.743352Z","shell.execute_reply":"2021-10-24T11:24:21.274987Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"It turns out that Ranking has a normal distribution, it is just that there are more restaurants in big cities, because of this we have an offset.","metadata":{}},{"cell_type":"code","source":"City_counts_dict = dict(df_train['City'].value_counts())\ndf_train['Restor_counts'] = df_train['City'].map(City_counts_dict)\ndf_train['Normalize_Ranking'] = df_train['Ranking']/df_train['Restor_counts']","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:21.277552Z","iopub.execute_input":"2021-10-24T11:24:21.278096Z","iopub.status.idle":"2021-10-24T11:24:21.295099Z","shell.execute_reply.started":"2021-10-24T11:24:21.278040Z","shell.execute_reply":"2021-10-24T11:24:21.293970Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"Eliminating Ranking's dependence on the number of restaurants in the city","metadata":{}},{"cell_type":"code","source":"for x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Normalize_Ranking'][df_train['City'] == x].hist(bins=35)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:21.296827Z","iopub.execute_input":"2021-10-24T11:24:21.297196Z","iopub.status.idle":"2021-10-24T11:24:22.347221Z","shell.execute_reply.started":"2021-10-24T11:24:21.297135Z","shell.execute_reply":"2021-10-24T11:24:22.346403Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"### Let's see the distribution of the target variable (Rating)","metadata":{}},{"cell_type":"code","source":"df_train['Rating'].value_counts(ascending=True).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:22.348455Z","iopub.execute_input":"2021-10-24T11:24:22.348887Z","iopub.status.idle":"2021-10-24T11:24:22.623637Z","shell.execute_reply.started":"2021-10-24T11:24:22.348823Z","shell.execute_reply":"2021-10-24T11:24:22.622479Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"### Let's see the distribution of the target variable Rating relative to the Ranking feature","metadata":{}},{"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:22.625469Z","iopub.execute_input":"2021-10-24T11:24:22.626063Z","iopub.status.idle":"2021-10-24T11:24:23.122000Z","shell.execute_reply.started":"2021-10-24T11:24:22.625997Z","shell.execute_reply":"2021-10-24T11:24:23.121221Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:23.124250Z","iopub.execute_input":"2021-10-24T11:24:23.124607Z","iopub.status.idle":"2021-10-24T11:24:23.606561Z","shell.execute_reply.started":"2021-10-24T11:24:23.124541Z","shell.execute_reply":"2021-10-24T11:24:23.605551Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"### Feature correlation\nIn this graph, you can now see how the traits are related to each other and to the target variable.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(df.drop(['sample'], axis=1).corr(),)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:23.607945Z","iopub.execute_input":"2021-10-24T11:24:23.608219Z","iopub.status.idle":"2021-10-24T11:24:25.068056Z","shell.execute_reply.started":"2021-10-24T11:24:23.608167Z","shell.execute_reply":"2021-10-24T11:24:25.066957Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\nNow, for convenience and code reproducibility, let's wrap all the processing in one big function.","metadata":{}},{"cell_type":"code","source":"# just in case, reload the data\nDATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # it's a train set\ndf_test['sample'] = 0 # it's a test set\ndf_test['Rating'] = 0 # in the test we do not have the Rating value, we must predict it, so for now we just fill it with zeros\n\ndf = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:25.070186Z","iopub.execute_input":"2021-10-24T11:24:25.070892Z","iopub.status.idle":"2021-10-24T11:24:25.388239Z","shell.execute_reply.started":"2021-10-24T11:24:25.070806Z","shell.execute_reply":"2021-10-24T11:24:25.387138Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n    \n    # ################### 1. Preprocessing ############################################################## \n    # we remove the features that are not necessary for the model\n    #df_output.drop(['Restaurant_id','ID_TA',], axis = 1, inplace=True)\n    df_output['Restaurant_id'] = df_output['Restaurant_id'].apply(lambda x: float(x[3:]))\n    df_output['ID_TA'] = df_output['ID_TA'].apply(lambda x: float(x[1:]))\n    \n    \n    # ################### 2. NAN ############################################################## \n    df_output['Number of Reviews'].fillna(0, inplace=True)\n    df_output['Number_of_Reviews_NaN'] = pd.isna(df_output['Number of Reviews']).astype('uint8')\n    df_output['Cuisine_Style_NaN'] = pd.isna(df_output['Cuisine Style']).astype('uint8')\n    df_output['Cuisine Style'] = df_output['Cuisine Style'].fillna(\"['Other']\")\n    df_output['Price_Range_NaN'] = pd.isna(df_output['Price Range']).astype('uint8')\n    df_output['Reviews'] = df_output['Reviews'].fillna('[[], []]')\n    df_output['Reviews_NaN'] = (df_output['Reviews'] == '[[], []]').astype('float64')\n    \n    # ################### 3. Encoding and Feature Engineering #########################################\n    \n    # Restaurant_id\n    show_chained_rest(df_output)\n    \n    # City\n    # Adding signs based on data about cities and countries\n    signs_for_cities(df_output)\n    \n    \n    \n    # Cuisine Style\n    df_output['Cuisine Style'] = df_output['Cuisine Style'].apply(str_to_list)\n    len_row(df_output)\n    vegan(df_output)\n    wine(df_output)\n    delicatessen(df_output)\n    fastfood(df_output)\n    sushi(df_output)\n    grill(df_output)\n    italian(df_output)\n    europe(df_output)\n    south_american(df_output)\n    asian(df_output)\n    healthy(df_output)\n    halal(df_output)\n    \n    # Ranking\n    ranking_normalize(df_output)\n    \n    # Price Range\n    #price_range(df_output)\n    df_output = pd.get_dummies(data=df_output, columns=['Price Range'], dummy_na=True)\n    \n    # Number of Reviews\n    df_output['Number of Reviews'].fillna((df_output['Number of Reviews'].mean()), inplace=True)\n    \n    # Reviews\n    days_bet_rev(df_output)\n    good_rev(df_output)\n\n    \n    # URL_TA\n    \n    # ID_TA\n    \n    # New signs\n    df_output['nor_population'] = df_output['Number of Reviews'] / df_output['population']*100000\n    df_output['rank_per_restaurant'] = df_output['Ranking']/df_output.all_rest\n    df_output['rank_per_city'] = df_output['Ranking']/df_output.population*1000\n    df_output['rest_per_popul'] = df_output.all_rest/df_output.population*1000\n    df_output['sol_per_popul_and_rest'] = (df_output.all_rest*df_output.average_salary)/df_output.population\n    #df_output['num_per_rank_per_city'] = df_output.rank_per_restaurant/df_output['Number of Reviews']\n   \n    df_output = pd.get_dummies(data=df_output, columns=['City',], dummy_na=True)\n    \n    \n    # ################### 4. Clean #################################################### \n    # remove signs that have not yet been processed,\n    # model on features with dtypes \"object\" will not be trained, just select them and delete\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    \n    # Feature standardization\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n\n    # Standardize all columns except target and Sample\n    for i in list(df_output.columns):\n        if i not in ['Rating','sample'] and i not in object_columns:\n            df_output[i] = StandardScaler_column(df_output, i)\n            if len(df_output[df_output[i].isna()]) < len(df_output):\n                df_output[i] = df_output[i].fillna(0)\n                \n                \n\n    return df_output\n\ndf_preproc = preproc_data(df)\ndf_preproc.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:25.393658Z","iopub.execute_input":"2021-10-24T11:24:25.393978Z","iopub.status.idle":"2021-10-24T11:24:28.250240Z","shell.execute_reply.started":"2021-10-24T11:24:25.393925Z","shell.execute_reply":"2021-10-24T11:24:28.249199Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":">On the good, it would be possible to translate this large function into a class and break it up into subfunctions (according to OOP).","metadata":{}},{"cell_type":"markdown","source":"#### Launch and check what happened","metadata":{}},{"cell_type":"code","source":"df_preproc = preproc_data(df)\ndf_preproc.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:28.251985Z","iopub.execute_input":"2021-10-24T11:24:28.252524Z","iopub.status.idle":"2021-10-24T11:24:30.990864Z","shell.execute_reply.started":"2021-10-24T11:24:28.252458Z","shell.execute_reply":"2021-10-24T11:24:30.989957Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"df_preproc.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:30.992722Z","iopub.execute_input":"2021-10-24T11:24:30.993089Z","iopub.status.idle":"2021-10-24T11:24:31.020167Z","shell.execute_reply.started":"2021-10-24T11:24:30.993032Z","shell.execute_reply":"2021-10-24T11:24:31.018777Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"# Now let's select the test part\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # our target\nX = train_data.drop(['Rating'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.021971Z","iopub.execute_input":"2021-10-24T11:24:31.022250Z","iopub.status.idle":"2021-10-24T11:24:31.074543Z","shell.execute_reply.started":"2021-10-24T11:24:31.022202Z","shell.execute_reply":"2021-10-24T11:24:31.073872Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"** Before sending our data for training, let's divide the data into one more test and train for validation.\nThis will help us test how well our model is performing before submitting the submission to kaggle. **","metadata":{}},{"cell_type":"code","source":"# Let's use the special function train_test_split to split test data\n# allocate 20% of the data for validation (parameter test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.075822Z","iopub.execute_input":"2021-10-24T11:24:31.076321Z","iopub.status.idle":"2021-10-24T11:24:31.097041Z","shell.execute_reply.started":"2021-10-24T11:24:31.076277Z","shell.execute_reply":"2021-10-24T11:24:31.096329Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# testing\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.098239Z","iopub.execute_input":"2021-10-24T11:24:31.098707Z","iopub.status.idle":"2021-10-24T11:24:31.104147Z","shell.execute_reply.started":"2021-10-24T11:24:31.098666Z","shell.execute_reply":"2021-10-24T11:24:31.103140Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"# Model \nСам ML","metadata":{}},{"cell_type":"code","source":"# Import the required libraries:\nfrom sklearn.ensemble import RandomForestRegressor # tool for creating and training a model\nfrom sklearn import metrics # tools for assessing model accuracy","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.105162Z","iopub.execute_input":"2021-10-24T11:24:31.105631Z","iopub.status.idle":"2021-10-24T11:24:31.118551Z","shell.execute_reply.started":"2021-10-24T11:24:31.105448Z","shell.execute_reply":"2021-10-24T11:24:31.117624Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Create a model (DO NOT TOUCH SETTINGS)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.119695Z","iopub.execute_input":"2021-10-24T11:24:31.119967Z","iopub.status.idle":"2021-10-24T11:24:31.134036Z","shell.execute_reply.started":"2021-10-24T11:24:31.119917Z","shell.execute_reply":"2021-10-24T11:24:31.132964Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# Train the model on the test dataset\nmodel.fit(X_train, y_train)\n\n# We use a trained model to predict restaurant ratings in a test sample.\n# Predicted values ​​are written to the y_pred variable\ny_pred = model.predict(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:31.135503Z","iopub.execute_input":"2021-10-24T11:24:31.135953Z","iopub.status.idle":"2021-10-24T11:24:48.445960Z","shell.execute_reply.started":"2021-10-24T11:24:31.135901Z","shell.execute_reply":"2021-10-24T11:24:48.445082Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# It can be observed that the difference in that real ratings are always multiples of 0.5\n# Write a function to round the predicted ratings accordingly\ndef round_rating_pred(rating_pred):\n    if rating_pred <= 0.25:\n        return 0.0\n    if rating_pred <= 0.75:\n        return 0.5\n    if rating_pred <= 1.25:\n        return 1.0\n    if rating_pred <= 1.75:\n        return 1.5\n    if rating_pred <= 2.25:\n        return 2.0\n    if rating_pred <= 2.75:\n        return 2.5\n    if rating_pred <= 3.25:\n        return 3.0\n    if rating_pred <= 3.75:\n        return 3.5\n    if rating_pred <= 4.25:\n        return 4.0\n    if rating_pred <= 4.75:\n        return 4.5\n    return 5.0\n\n\n# Round it\nfor i in range(len(y_pred)):\n    y_pred[i] = round_rating_pred(y_pred[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:48.447633Z","iopub.execute_input":"2021-10-24T11:24:48.447944Z","iopub.status.idle":"2021-10-24T11:24:48.465139Z","shell.execute_reply.started":"2021-10-24T11:24:48.447889Z","shell.execute_reply":"2021-10-24T11:24:48.464323Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# Compare the predicted values ​​(y_pred) with the real ones (y_test), and see how much they differ on average\n# The metric is called the Mean Absolute Error (MAE) and shows the average deviation of the predicted values ​​from the actual ones.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:48.466406Z","iopub.execute_input":"2021-10-24T11:24:48.466871Z","iopub.status.idle":"2021-10-24T11:24:48.485978Z","shell.execute_reply.started":"2021-10-24T11:24:48.466813Z","shell.execute_reply":"2021-10-24T11:24:48.485024Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"# in RandomForestRegressor it is possible to display the most important features for the model\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:48.487966Z","iopub.execute_input":"2021-10-24T11:24:48.488744Z","iopub.status.idle":"2021-10-24T11:24:48.957922Z","shell.execute_reply.started":"2021-10-24T11:24:48.488670Z","shell.execute_reply":"2021-10-24T11:24:48.956997Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nГотовим Submission на Kaggle","metadata":{}},{"cell_type":"code","source":"test_data.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:48.959429Z","iopub.execute_input":"2021-10-24T11:24:48.959982Z","iopub.status.idle":"2021-10-24T11:24:49.021473Z","shell.execute_reply.started":"2021-10-24T11:24:48.959923Z","shell.execute_reply":"2021-10-24T11:24:49.020521Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:49.023173Z","iopub.execute_input":"2021-10-24T11:24:49.023750Z","iopub.status.idle":"2021-10-24T11:24:49.033447Z","shell.execute_reply.started":"2021-10-24T11:24:49.023605Z","shell.execute_reply":"2021-10-24T11:24:49.031867Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:49.035250Z","iopub.execute_input":"2021-10-24T11:24:49.035700Z","iopub.status.idle":"2021-10-24T11:24:49.056313Z","shell.execute_reply.started":"2021-10-24T11:24:49.035499Z","shell.execute_reply":"2021-10-24T11:24:49.055275Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"predict_submission = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:49.057524Z","iopub.execute_input":"2021-10-24T11:24:49.057949Z","iopub.status.idle":"2021-10-24T11:24:49.267965Z","shell.execute_reply.started":"2021-10-24T11:24:49.057891Z","shell.execute_reply":"2021-10-24T11:24:49.267113Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"predict_submission = list(map(round_rating_pred, predict_submission))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:49.269379Z","iopub.execute_input":"2021-10-24T11:24:49.269734Z","iopub.status.idle":"2021-10-24T11:24:49.283986Z","shell.execute_reply.started":"2021-10-24T11:24:49.269664Z","shell.execute_reply":"2021-10-24T11:24:49.282880Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T11:24:49.285775Z","iopub.execute_input":"2021-10-24T11:24:49.286163Z","iopub.status.idle":"2021-10-24T11:24:49.328567Z","shell.execute_reply.started":"2021-10-24T11:24:49.286091Z","shell.execute_reply":"2021-10-24T11:24:49.327596Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}